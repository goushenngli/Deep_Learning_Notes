 激活函数是向神经网络中引入非线性因素，通过激活函数神经网络就可以拟合各种曲线。
 激活函数有以下几大特性：
	1. 激活函数必须是非线性的
	2. 激活函数必须是单调的S型曲线
	3. 激活函数必须是连续可导的
	4. 激活函数定义阈为R(所有实数)

 激活函数主要分为饱和激活函数（Saturated Neurons）和非饱和函数（One-sided Saturations）。Sigmoid和Tanh是饱和激活函数，而ReLU以及其变种为非饱和激活函数。非饱和激活函数主要有如下优势：
1. 非饱和激活函数可以解决梯度消失问题。
2. 非饱和激活函数可以加速收敛。
# sigmoid函数
sigmoid函数属于饱和函数。
 当线性输出单元y的值非常大或者非常小的时候，其导数接近于0。而其导数的最大值为0.25,这就意味着神经网络进行反响传播的时候每层梯度都会缩小大约1/4。

[[梯度消失]]
[小萝卜sigmoid函数介绍](https://www.bilibili.com/video/BV1qB4y1e7GJ?t=98.9)

**sigmoid函数(左)和sigmoid的导数(右)**[![jzEipn.png](https://s1.ax1x.com/2022/07/26/jzEipn.png)](https://imgtu.com/i/jzEipn)

# 参考文献
